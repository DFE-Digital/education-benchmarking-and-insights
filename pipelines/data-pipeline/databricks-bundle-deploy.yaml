trigger:
  branches:
    include:
      - main
      - feature/databricks-notebooks

pr:
  - main

variables:
  - group: databricks-vars
  - name: PYTHON_VERSION
    value: '3.12'

stages:
  - stage: Test
    displayName: 'Test and create artifact'
    jobs:
      - job: unit_tests
        displayName: 'Unit Tests'
        pool:
          vmImage: 'ubuntu-latest'
        steps:
          - task: UsePythonVersion@0
            inputs:
              versionSpec: '$(PYTHON_VERSION)'
            displayName: 'Use Python $(PYTHON_VERSION)'

          - script: |
              sudo apt-get update
              sudo apt-get install temurin-17-jdk
            displayName: 'Install Java 17'

          - script: |
              curl -LsSf https://astral.sh/uv/install.sh | sh
              echo "##vso[task.prependpath]$HOME/.cargo/bin"
            displayName: 'Install uv'

          - script: |
              uv sync --group unit-test
            workingDirectory: $(System.DefaultWorkingDirectory)/databricks/education_benchmarking_and_insights
            displayName: 'Install Dependencies with uv'

          # - script: |
          #     uv run pytest -v
          #   workingDirectory: $(System.DefaultWorkingDirectory)/databricks/education_benchmarking_and_insights
          #   displayName: 'Run unit tests'
          #   continueOnError: false

      - job: prepare_artifacts
        displayName: 'Prepare artifacts for the restricted windows deployment runner'
        pool:
          vmImage: 'ubuntu-latest'
        steps:
          - task: CopyFiles@2
            displayName: 'Copy source files to staging'
            inputs:
              SourceFolder: '$(Build.SourcesDirectory)'
              Contents: |
                databricks/**
                !databricks/**/.pytest_cache/**
                !databricks/**/__pycache__/**
                !databricks/**/.venv/**
                !databricks/tf_binaries.zip
              TargetFolder: '$(Build.ArtifactStagingDirectory)'
              CleanTargetFolder: true

          - script: |
              unzip $(System.DefaultWorkingDirectory)/databricks/tf_binaries.zip -d $(Build.ArtifactStagingDirectory)
            displayName: 'Prepare Windows terraform/databricks binaries'

          - script: |
              curl -fsSL https://github.com/databricks/cli/releases/download/v0.276.0/databricks_cli_0.276.0_windows_amd64.zip -o $(Build.ArtifactStagingDirectory)/databricks-cli.zip
              unzip $(Build.ArtifactStagingDirectory)/databricks-cli.zip -d $(Build.ArtifactStagingDirectory)/databricks-cli
              rm $(Build.ArtifactStagingDirectory)/databricks-cli.zip
            displayName: 'Download Windows Databricks CLI binary'

          - script: |
              curl -LsSf https://github.com/astral-sh/uv/releases/latest/download/uv-x86_64-pc-windows-msvc.zip -o $(Build.ArtifactStagingDirectory)/uv.zip
              unzip $(Build.ArtifactStagingDirectory)/uv.zip -d $(Build.ArtifactStagingDirectory)/uv
              rm $(Build.ArtifactStagingDirectory)/uv.zip
            displayName: 'Download uv for Windows'

          - task: PublishBuildArtifacts@1
            displayName: 'Publish Artifact'
            inputs:
              PathtoPublish: '$(Build.ArtifactStagingDirectory)'
              ArtifactName: '$(artifactName)'
              publishLocation: 'Container'

      - job: Bundle_Validate
        pool:
          name: DD-Hosted-Prod
        dependsOn: unit_tests
        steps:
          - checkout: none
            displayName: 'Skip checkout (using artifact)'

          - task: DownloadBuildArtifacts@1
            displayName: 'Download source code artifact'
            inputs:
              buildType: 'current'
              downloadType: 'single'
              artifactName: '$(artifactName)'
              downloadPath: '$(System.ArtifactsDirectory)'
          
          - script: |
              robocopy $(System.ArtifactsDirectory)\$(artifactName)\tf_binaries C:/temp/tf_binaries
              robocopy $(System.ArtifactsDirectory)\$(artifactName)\databricks %APPDATA% terraform.rc
            displayName: 'Configure terraform to use local databricks provider'

          - script: |
              databricks bundle validate
            workingDirectory: $(System.ArtifactsDirectory)\$(artifactName)\databricks\education_benchmarking_and_insights
            displayName: 'Validate Bundle Configuration'
            env:
              PATH: $(System.ArtifactsDirectory)\$(artifactName)\databricks-cli;$(PATH)
              DATABRICKS_HOST: $(DATABRICKS_HOST)
              DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)

          - script: |
              ipconfig
              databricks bundle deploy --target dev --log-level debug
            displayName: 'Validate Bundle Configuration'
            workingDirectory: $(System.ArtifactsDirectory)\$(artifactName)\databricks\education_benchmarking_and_insights
            env:
              PATH: $(System.ArtifactsDirectory)\$(artifactName)\uv;$(System.ArtifactsDirectory)\$(artifactName)\databricks-cli;$(PATH)
              DATABRICKS_HOST: $(DATABRICKS_HOST)
              DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)

  - stage: DeployDev
    displayName: 'Deploy to Dev'
    dependsOn: Test
    condition: and(succeeded(), in(variables['Build.SourceBranch'], 'refs/heads/develop', 'refs/heads/main'))
    jobs:
      - deployment: DeployDevBundle
        displayName: 'Deploy to Dev Environment'
        environment: 'development'
        strategy:
          runOnce:
            deploy:
              steps:
                - checkout: self

                - task: UsePythonVersion@0
                  inputs:
                    versionSpec: '$(PYTHON_VERSION)'
                  displayName: 'Use Python $(PYTHON_VERSION)'

                - script: |
                    curl -LsSf https://astral.sh/uv/install.sh | sh
                    echo "##vso[task.prependpath]$HOME/.cargo/bin"
                  displayName: 'Install uv'

                - script: |
                    uv sync
                  displayName: 'Install Dependencies with uv'

                - script: |
                    python -m pip install databricks-cli
                  displayName: 'Install Databricks CLI'

                # Generate version from commit SHA
                - script: |
                    export GIT_COMMIT=$(git rev-parse --short HEAD)
                    export BRANCH_NAME=$(echo $(Build.SourceBranch) | sed 's/refs\/heads\///')
                    export BUNDLE_VERSION="${BRANCH_NAME}-${GIT_COMMIT}"
                    echo "##vso[task.setvariable variable=BUNDLE_VERSION]${BUNDLE_VERSION}"
                    echo "##vso[task.setvariable variable=GIT_COMMIT]${GIT_COMMIT}"
                  displayName: 'Generate Bundle Version'

                - script: |
                    databricks bundle deploy --target dev
                  displayName: 'Deploy Bundle to Dev'
                  env:
                    DATABRICKS_HOST: $(DATABRICKS_HOST)
                    DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)

                - script: |
                    echo "Bundle deployed successfully to dev"
                    databricks bundle summary --target dev
                  displayName: 'Deployment Summary'
                  env:
                    DATABRICKS_HOST: $(DATABRICKS_HOST)
                    DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)

  # - stage: DeployProd
  #   displayName: 'Deploy to Production'
  #   dependsOn: DeployDev
  #   condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'))
  #   jobs:
  #     - deployment: DeployProdBundle
  #       displayName: 'Deploy to Production Environment'
  #       environment: 'development'
  #       strategy:
  #         runOnce:
  #           deploy:
  #             steps:
  #               - checkout: self

  #               - task: UsePythonVersion@0
  #                 inputs:
  #                   versionSpec: '$(PYTHON_VERSION)'
  #                 displayName: 'Use Python $(PYTHON_VERSION)'

  #               - script: |
  #                   curl -LsSf https://astral.sh/uv/install.sh | sh
  #                   echo "##vso[task.prependpath]$HOME/.cargo/bin"
  #                 displayName: 'Install uv'

  #               - script: |
  #                   uv sync
  #                 displayName: 'Install Dependencies with uv'

  #               - script: |
  #                   python -m pip install databricks-cli
  #                 displayName: 'Install Databricks CLI'

  #               - script: |
  #                   databricks bundle deploy --target prod
  #                 displayName: 'Deploy Bundle to Production'
  #                 env:
  #                   DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)

  #               - script: |
  #                   echo "Bundle deployed successfully to production"
  #                   databricks bundle summary --target prod
  #                 displayName: 'Deployment Summary'
  #                 env:
  #                   DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)